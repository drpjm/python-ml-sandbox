{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Concepts and API\n",
    "\n",
    "All TF code follows this process:\n",
    "1. Create a **computation graph** that defines your computational structure\n",
    "2. Create a TF session\n",
    "3. Run the computation graph in the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# Define variables and operations in the graph\n",
    "\n",
    "x = tf.Variable(3, name=\"x\") # declare a symbolic name, x\n",
    "y = tf.Variable(4, name=\"y\")\n",
    "g = x*x*y\n",
    "h = y**3\n",
    "print(type(g))\n",
    "print(type(h))\n",
    "f = g + h\n",
    "print(type(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *type* of each computation is a TF **op**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a session and run. Using the \"with\" context block automatically closes the session.\n",
    "with tf.Session() as tf_sess:\n",
    "    x.initializer.run()\n",
    "    y.initializer.run()\n",
    "    result = f.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative to initializing variables individually is to call the <code>global_variables_initializer</code> function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer() # Creates an init node\n",
    "\n",
    "with tf.Session() as tf_sess:\n",
    "    init.run()\n",
    "    result = f.eval()\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs\n",
    "\n",
    "We can build graphs and then merge them together programmatically. Otherwise, it is assumed that declared computations are applied to the **same graph**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = tf.Variable(1)\n",
    "# check where this x1 node lives:\n",
    "x1.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Now, make another graph and add a new variable to it:\n",
    "new_graph = tf.Graph()\n",
    "with new_graph.as_default():\n",
    "    x2 = tf.Variable(2)\n",
    "    \n",
    "print(x2.graph is tf.get_default_graph())\n",
    "print(x2.graph is new_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More on Nodes\n",
    "\n",
    "TF node evaluation determines the set of nodes that the node depends on and evaluates them. **All node values (except variables) are dropped between graph runs!**\n",
    "\n",
    "Varialbes start their life when initialized and end when the session closes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "3969\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "w = tf.constant(9)\n",
    "x = w * 7\n",
    "y = x + 2\n",
    "z = x**2\n",
    "\n",
    "with tf.Session() as tf_sess:\n",
    "    print(y.eval())\n",
    "    print(z.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code is not efficient, as the computation of x and w will happen twice! Instead, have y and evaluate in a single graph run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "3969\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as tf_sess:\n",
    "    y_val, z_val = tf_sess.run([y,z])\n",
    "    print(y_val)\n",
    "    print(z_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations\n",
    "\n",
    "TF \"ops\" can take *any* number of inputs and produce *any* number of outputs. Sources are constants and Variables. The inputs and outputs of operations are always **tensors** - multi-dimensional arrays. In TF, tensors are numpy `ndarray`s.\n",
    "\n",
    "The following example performs linear regression using the closed form Normal Equation embedded as a TF op. \n",
    "This example code uses the California housing data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: 20640 instances, 8 features\n",
      "Training size: 16512; Test size: 4128\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "housing_dataset = fetch_california_housing()\n",
    "m,n = housing_dataset.data.shape\n",
    "print(\"Data shape: \" + str(m) + \" instances, \" + str(n) + \" features\")\n",
    "\n",
    "X_raw = housing_dataset.data\n",
    "y_raw = housing_dataset.target\n",
    "\n",
    "# Split up the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_raw, y_raw, test_size=0.2)\n",
    "print(\"Training size: \" + str(X_train.shape[0]) + \"; Test size: \" + str(X_test.shape[0]))\n",
    "\n",
    "# Scale the data sets\n",
    "housing_scaler = StandardScaler()\n",
    "X_train_scaled = housing_scaler.fit_transform(X_train)\n",
    "X_test_scaled = housing_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biased train data shape: 16512 instances, 9 features\n",
      "Biased test data shape: 4128 instances, 9 features\n"
     ]
    }
   ],
   "source": [
    "# Add a bias of 1 to model the linear regression.\n",
    "X_train_biased = np.c_[np.ones((X_train_scaled.shape[0],1)), X_train_scaled]\n",
    "X_test_biased = np.c_[np.ones((X_test_scaled.shape[0],1)), X_test_scaled]\n",
    "print(\"Biased train data shape: \" + str(X_train_biased.shape[0]) + \" instances, \" + str(X_train_biased.shape[1]) + \" features\")\n",
    "print(\"Biased test data shape: \" + str(X_test_biased.shape[0]) + \" instances, \" + str(X_test_biased.shape[1]) + \" features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target array shape: (16512, 9)\n",
      "...as TF constant: (16512,)\n"
     ]
    }
   ],
   "source": [
    "X = tf.constant(X_train_biased, dtype=tf.float32, name=\"X\")\n",
    "print(\"Target array shape: \" + str(X_train_biased.shape))\n",
    "# Explicitly turn into an m x 1 vector\n",
    "y = tf.constant(y_train.reshape(-1,1), dtype=tf.float32, name=\"y\")\n",
    "print(\"...as TF constant: \" + str(y_train.shape))\n",
    "XT = tf.transpose(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the **Normal Equation**:\n",
    "$\\theta^{\\star} = (X\\cdot X^T)^{-1}\\cdot{X^T}\\cdot{y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv = tf.matrix_inverse( tf.matmul(XT, X) )\n",
    "theta = tf.matmul( tf.matmul(inv, XT), y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vroom vroom!\n",
    "with tf.Session() as tf_sess:\n",
    "    theta_val = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed a linear regression over the data set:\n",
      "[[ 2.0732806e+00]\n",
      " [ 8.4796059e-01]\n",
      " [ 1.1729030e-01]\n",
      " [-2.7714539e-01]\n",
      " [ 3.2005358e-01]\n",
      " [-1.8042531e-03]\n",
      " [-4.1402280e-02]\n",
      " [-9.2463315e-01]\n",
      " [-8.9791656e-01]]\n",
      " (9, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Performed a linear regression over the data set:\")\n",
    "print(str(theta_val) + \"\\n \" + str(theta_val.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Gradient Descent via TF\n",
    "\n",
    "I will re-use the scaled data from above and implement gradient descent manually rather than use the normal equation solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = X_train_biased.shape[0]\n",
    "n = X_train_biased.shape[1]\n",
    "\n",
    "n_epochs = 2000\n",
    "alpha = 0.01 # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For grins, make a new graph for this implementation.\n",
    "gd_graph = tf.Graph()\n",
    "\n",
    "with gd_graph.as_default():\n",
    "    X = tf.constant(X_train_biased, dtype=tf.float32, name=\"X\")\n",
    "    y = tf.constant(y_train.reshape(-1,1), dtype=tf.float32, name=\"y\")\n",
    "    # Initialize theta variables with uniform random values\n",
    "    theta = tf.Variable( tf.random_uniform([n, 1], -1.0, 1.0), name=\"theta\" )\n",
    "    # Compute the predictions and error\n",
    "    y_pred = tf.matmul( X, theta, name=\"predictions\" )\n",
    "    error = y_pred - y\n",
    "    # Call on TF's mse function\n",
    "    mse = tf.reduce_mean( tf.square(error), name=\"mse\" )\n",
    "    # Gradient calculations\n",
    "    dJdtheta = (2.0/m) * tf.matmul( tf.transpose(X), error )\n",
    "    # Training/learning op. assign() computes a new value and assigns it to a TF variable\n",
    "    train_op = tf.assign( theta, theta - alpha*dJdtheta )\n",
    "    \n",
    "    init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 MSE =  5.955667\n",
      "Epoch  100 MSE =  0.667297\n",
      "Epoch  200 MSE =  0.57017946\n",
      "Epoch  300 MSE =  0.5572751\n",
      "Epoch  400 MSE =  0.54890317\n",
      "Epoch  500 MSE =  0.54260564\n",
      "Epoch  600 MSE =  0.53780556\n",
      "Epoch  700 MSE =  0.53412336\n",
      "Epoch  800 MSE =  0.5312809\n",
      "Epoch  900 MSE =  0.52907455\n",
      "Epoch  1000 MSE =  0.5273521\n",
      "Epoch  1100 MSE =  0.5260012\n",
      "Epoch  1200 MSE =  0.52493477\n",
      "Epoch  1300 MSE =  0.52408993\n",
      "Epoch  1400 MSE =  0.5234178\n",
      "Epoch  1500 MSE =  0.5228803\n",
      "Epoch  1600 MSE =  0.52244985\n",
      "Epoch  1700 MSE =  0.5221027\n",
      "Epoch  1800 MSE =  0.5218228\n",
      "Epoch  1900 MSE =  0.5215965\n",
      "[[ 2.0732749e+00]\n",
      " [ 8.7297672e-01]\n",
      " [ 1.2570815e-01]\n",
      " [-3.1378302e-01]\n",
      " [ 3.4614247e-01]\n",
      " [ 9.1348443e-04]\n",
      " [-4.2238470e-02]\n",
      " [-8.4004754e-01]\n",
      " [-8.1584382e-01]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session( graph=gd_graph ) as sess:\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    for i in range(n_epochs):\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(\"Epoch \", i, \"MSE = \", mse.eval())\n",
    "        sess.run(train_op)\n",
    "    \n",
    "    # At the end, print the current thetas\n",
    "    print(theta.eval())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are pretty good compared to the normal equation. But it would be nice to not have to compute the derivative by hand all the time, especially for more difficult functions, e.g. regularized cost functions. Next, I will use *autodiff* to automatically compute the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"dJdtheta/predictions_grad/MatMul_1:0\", shape=(9, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "gd_graph2 = tf.Graph()\n",
    "with gd_graph2.as_default():\n",
    "    X = tf.constant(X_train_biased, dtype=tf.float32, name=\"X\")\n",
    "    y = tf.constant(y_train.reshape(-1,1), dtype=tf.float32, name=\"y\")\n",
    "    # Initialize theta variables with uniform random values\n",
    "    theta = tf.Variable( tf.random_uniform([n, 1], -1.0, 1.0), name=\"theta\" )\n",
    "    # Compute the predictions and error\n",
    "    y_pred = tf.matmul( X, theta, name=\"predictions\" )\n",
    "    error = y_pred - y\n",
    "    # Call on TF's mse function\n",
    "    mse = tf.reduce_mean( tf.square(error), name=\"mse\" )\n",
    "    # Using tf's autodiff capability compute the derivative of the MSE\n",
    "    dJdtheta = tf.gradients( mse, [theta], name=\"dJdtheta\" )[0]\n",
    "    print(dJdtheta)\n",
    "    \n",
    "    # Training/learning op. assign() computes a new value and assigns it to a TF variable\n",
    "    # This *is* the optimization process - simple gradient descent\n",
    "    train_op = tf.assign( theta, theta - alpha*dJdtheta )\n",
    "    \n",
    "    init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 MSE =  7.85264\n",
      "Epoch  100 MSE =  0.77685225\n",
      "Epoch  200 MSE =  0.65164685\n",
      "Epoch  300 MSE =  0.6166419\n",
      "Epoch  400 MSE =  0.59215075\n",
      "Epoch  500 MSE =  0.5742215\n",
      "Epoch  600 MSE =  0.5610168\n",
      "Epoch  700 MSE =  0.55124444\n",
      "Epoch  800 MSE =  0.543973\n",
      "Epoch  900 MSE =  0.5385329\n",
      "Epoch  1000 MSE =  0.53444034\n",
      "Epoch  1100 MSE =  0.5313431\n",
      "Epoch  1200 MSE =  0.52898556\n",
      "Epoch  1300 MSE =  0.5271791\n",
      "Epoch  1400 MSE =  0.52578795\n",
      "Epoch  1500 MSE =  0.52471\n",
      "Epoch  1600 MSE =  0.52386934\n",
      "Epoch  1700 MSE =  0.523211\n",
      "Epoch  1800 MSE =  0.52269274\n",
      "Epoch  1900 MSE =  0.52228254\n",
      "[[ 2.0732749e+00]\n",
      " [ 8.7503898e-01]\n",
      " [ 1.2893695e-01]\n",
      " [-3.1248391e-01]\n",
      " [ 3.4260067e-01]\n",
      " [ 2.0070914e-03]\n",
      " [-4.2432308e-02]\n",
      " [-8.1376356e-01]\n",
      " [-7.8953385e-01]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session( graph=gd_graph2 ) as sess:\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    for i in range(n_epochs):\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(\"Epoch \", i, \"MSE = \", mse.eval())\n",
    "        sess.run(train_op)\n",
    "    \n",
    "    # At the end, print the current thetas\n",
    "    print(theta.eval())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to roll all of the above into a simple call to a tf `Optimizer`! \n",
    "The following code includes logging (`tf.summary`) and model saving commands (`tf.train.Saver`) as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime # for logging\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "logdir = \"./logs/run-{0}\".format(now)\n",
    "\n",
    "gdwithopt_graph = tf.Graph()\n",
    "# All the same intialization code, but then call on a MomentumOptimizer (or whatever other flavor)\n",
    "with gdwithopt_graph.as_default():\n",
    "    X = tf.constant(X_train_biased, dtype=tf.float32, name=\"X\")\n",
    "    y = tf.constant(y_train.reshape(-1,1), dtype=tf.float32, name=\"y\")\n",
    "    # Initialize theta variables with uniform random values\n",
    "    theta = tf.Variable( tf.random_uniform([n, 1], -1.0, 1.0), name=\"theta\" )\n",
    "    # Compute the predictions and error\n",
    "    y_pred = tf.matmul( X, theta, name=\"y_pred\" )\n",
    "    with tf.name_scope(\"loss\") as scope:\n",
    "        error = y_pred - y\n",
    "        # Call on TF's mse function\n",
    "        mse = tf.reduce_mean( tf.square(error), name=\"mse\" )\n",
    "    \n",
    "    # The optimizer:\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=alpha)\n",
    "    # Uncomment to use MomentumOptimizer\n",
    "#     optimizer = tf.train.MomentumOptimizer(learning_rate=alpha, momentum=0.9)\n",
    "\n",
    "    training_op = optimizer.minimize(mse)\n",
    "    \n",
    "    init_op = tf.global_variables_initializer()\n",
    "    \n",
    "    # Added a saver node\n",
    "    saver = tf.train.Saver()\n",
    "    # Logging for Tensorboard\n",
    "    mse_summary = tf.summary.scalar('MSE',mse) # Creates a node that outputs value of mse\n",
    "    file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.0732749 ]\n",
      " [ 0.8661438 ]\n",
      " [ 0.13141075]\n",
      " [-0.29014128]\n",
      " [ 0.32104757]\n",
      " [ 0.00291607]\n",
      " [-0.04240436]\n",
      " [-0.8021911 ]\n",
      " [-0.7765645 ]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=gdwithopt_graph) as sess:\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    for i in range(n_epochs):\n",
    "        if i % 50 == 0:\n",
    "            # Log current MSE\n",
    "            file_writer.add_summary(mse_summary.eval(), i)\n",
    "            # Save a model checkpoint\n",
    "            save_path = saver.save(sess, \"./models/lin_reg_mid.ckpt\")\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    # At the end, print the current thetas\n",
    "    print(theta.eval())\n",
    "    # Save out the model\n",
    "    save_path = saver.save(sess, \"./models/lin_reg_final.ckpt\")\n",
    "    file_writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code still performed a batch learning process: the whole data set was consumed and the model was trained. Next step in thie evolution is to move to a **mini-batch** process. $X$ and $y$ get replaced with new values from the data set on each epoch iteration. In TF, we use `placeholder` nodes to accomplish this modification. (Note that `None` means any size.)\n",
    "\n",
    "To pass in a value to placeholder nodes, create a `feed_dict` and assign the value with the variable as a keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import randn\n",
    "\n",
    "placeholder_ex_graph = tf.Graph()\n",
    "with placeholder_ex_graph.as_default():\n",
    "    A = tf.placeholder(tf.float32, shape=(None,4), name='A')\n",
    "    B = tf.placeholder(tf.int64, shape=(None,6), name='B')\n",
    "    C = A * 2 \n",
    "    D = B + 2\n",
    "\n",
    "with tf.Session(graph=placeholder_ex_graph) as sess:\n",
    "    C_result = C.eval( feed_dict={A: randn(1,4)} )\n",
    "    D_result = D.eval( feed_dict={B: [[8,9,1,2,7,3]]} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.2006731  2.7963092  1.562574  -0.6011623]]\n",
      "[[10 11  3  4  9  5]]\n"
     ]
    }
   ],
   "source": [
    "print(C_result)\n",
    "print(D_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow has a batching function, `tf.train.batch`. See the details [here](https://www.tensorflow.org/api_docs/python/tf/train/batch).\n",
    "\n",
    "A quick review of the input data shapes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16512, 9)\n",
      "Sample X:\n",
      "[ 1.          0.9144781   0.5066612   0.49843741 -0.28258584  0.00519773\n",
      "  0.00825509 -0.84464416  0.83965343]\n",
      "(16512,)\n",
      "Sample y:\n",
      "2.008\n"
     ]
    }
   ],
   "source": [
    "print(X_train_biased.shape)\n",
    "print(\"Sample X:\\n{0}\".format(X_train_biased[0]))\n",
    "print(y_train.shape)\n",
    "print(\"Sample y:\\n{0}\".format(y_train[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches = 65\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "n_batches = int(np.ceil(X_train_biased.shape[0] / batch_size))\n",
    "print(\"Number of batches = {0}\".format(n_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "def next_batch_rand(Xs,ys,batch_size):\n",
    "    \"\"\"\n",
    "    This function randomly samples from the training set based on the size of the batch.\n",
    "    \"\"\"\n",
    "    data_len = Xs.shape[0]\n",
    "    idxs = sample( list(range(0,data_len)), batch_size )\n",
    "    return Xs[idxs,:], ys[idxs].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 9)\n",
      "(16, 1)\n"
     ]
    }
   ],
   "source": [
    "X_b, y_b = next_batch_rand(X_train_biased, y_train, 16)\n",
    "print(X_b.shape)\n",
    "print(y_b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini-batch Gradient Descent!\n",
    "gd_minibatch = tf.Graph()\n",
    "with gd_minibatch.as_default():\n",
    "    \n",
    "    # Now X and y are fed into the graph. n is the number of features (dimensions) in X\n",
    "    X = tf.placeholder( tf.float32, shape=(None, n), name='X' )\n",
    "    y = tf.placeholder( tf.float32, shape=(None, 1), name='y')\n",
    "\n",
    "    # Initialize theta variables with uniform random values\n",
    "    theta = tf.Variable( tf.random_uniform([n, 1], -1.0, 1.0), name=\"theta\" )\n",
    "    # Compute the predictions and error\n",
    "    y_pred = tf.matmul( X, theta, name=\"y_pred\" )\n",
    "    error = y_pred - y\n",
    "    # Call on TF's mse function\n",
    "    mse = tf.reduce_mean( tf.square(error), name=\"mse\" )\n",
    "    \n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate=alpha, momentum=0.9)\n",
    "\n",
    "    training_op = optimizer.minimize(mse)\n",
    "    \n",
    "    init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am not sure why I get an exception with the following code. I get an `InvalidArgumentError` because of the `y` being the wrong shape. But my batch function always returns a `(?,1)` sized array, based on the size of the batch. It appears to be fine with the shape of `X_batch`.\n",
    "\n",
    "The exact error is:\n",
    "```\n",
    "You must feed a value for placeholder tensor 'y' with dtype float and shape [?,1]\n",
    "\t [[Node: y = Placeholder[dtype=DT_FLOAT, shape=[?,1], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
    "```\n",
    "According to docs, values passed into the `feed_dict` with `(?,n)` shape requirement must have rank 2 (2d array).\n",
    "\n",
    "**Issue resolved**: I did not pass in the `feed_dict` into `mse.eval()`...*facepalm*. That said, my random batch generation appears to be causing instabilities in the mini-batch training! Will return to fix later...\n",
    "\n",
    "Key takeaway: ensure that the data passed into the `feed_dict` is a numpy array with rank 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.Session(graph=gd_minibatch) as sess:\n",
    "#     sess.run(init_op)\n",
    "    \n",
    "#     for i in range(n_epochs):\n",
    "#         # Within each epoch, train over all mini batches in the training data\n",
    "#         for batch_idx in range(n_batches):\n",
    "#             X_batch, y_batch = next_batch_rand(X_train_biased, y_train, batch_size=batch_size)\n",
    "#             print(batch_idx)\n",
    "#             print(X_batch.shape)\n",
    "#             print(y_batch.shape)\n",
    "#             if batch_idx % 64 == 0:\n",
    "#                 print(y_batch)\n",
    "#             sess.run(training_op, feed_dict={X:X_batch, y:y_batch})\n",
    "#             print(\"Ran...\")\n",
    "#         if i % 100 == 0:\n",
    "#             print(\"Epoch \", i, \"MSE = \", mse.eval(feed_dict={X:X_batch, y:y_batch}))\n",
    "\n",
    "#     print(theta.eval())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Graph Construction Tools\n",
    "\n",
    "TF allows you to define functions over the tf components. It also has facilities to share variables among graph components.\n",
    "\n",
    "Let's make a modular rectified linear unit (ReLU) from scratch.\n",
    "A ReLU outputs the maximum of a linear combination of features ($X$) and weights ($w$) or 0:\n",
    "\n",
    "$$\n",
    "h_{w,b}(X) = max(X\\cdot w + b, 0)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    \"\"\"X is an input feature tensor.\"\"\"\n",
    "    with tf.name_scope(\"relu\") as scope:\n",
    "        with tf.variable_scope(\"relu\", reuse=True): # allow the reuse of a variable outside of scope\n",
    "            w_shape = (int(X.get_shape()[1]), 1) # w must align with X's columns\n",
    "            w = tf.Variable(tf.random_normal(w_shape), name=\"w\")\n",
    "            threshold = tf.get_variable(\"threshold\")\n",
    "#             b = tf.Variable(0.0, name=\"b\")\n",
    "            z = tf.add( tf.matmul(X,w), threshold, name=\"z\" )\n",
    "#             z = tf.add( tf.matmul(X,w), b, name=\"z\" )\n",
    "        return tf.maximum(z, 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = \"./logs/relu-{0}\".format(datetime.utcnow().strftime(\"%Y%m%d%H%M%S\"))\n",
    "\n",
    "n_features = 4\n",
    "\n",
    "relu_graph = tf.Graph()\n",
    "with relu_graph.as_default():\n",
    "    X = tf.placeholder( tf.float32, shape=(None,n_features), name=\"X\" )\n",
    "    # Create the relu variable scope and define a threshold value\n",
    "    with tf.variable_scope(\"relu\"):\n",
    "        threshold = tf.get_variable(\"threshold\", shape=(),\n",
    "                                   initializer=tf.constant_initializer(0.0))\n",
    "    relus = [relu(X) for i in range(6)]\n",
    "    output = tf.add_n(relus, name=\"output\") # Add results of all n relu tensors\n",
    "    \n",
    "    init_op = tf.global_variables_initializer()\n",
    "    file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output = [[6.4203606]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=relu_graph) as sess:\n",
    "    sess.run(init_op)\n",
    "    result = output.eval( feed_dict = {X:randn(1,n_features)} )\n",
    "    print(\"Output = {0}\".format(result))\n",
    "    file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
